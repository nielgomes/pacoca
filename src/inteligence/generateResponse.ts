//src/inteligence/generateResponse.ts
import { ChatCompletionMessageParam } from "openai/resources";
import { openai } from "../services/openai";
import { Data } from "../utils/database";
import PERSONALITY_PROMPT from "../constants/PERSONALITY_PROMPT";
import beautifulLogger from "../utils/beautifulLogger";
import config from '../../model.json';
import mediaCatalog from '../../media_catalog.json';
import { Message, Action, BotResponse, GenerateResponseResult } from "./types";


// --- Constantes Centralizadas ---
// ATUALIZA√á√ÉO: Alterado para o modelo gratuito DeepSeek da OpenRouter.
const MODEL_NAME = config.exp.MODEL_NAME;
const MODEL_PRICING = {
  // Geralmente informado em USD$ por Milh√£o de tokens
  // se o modelo √© gratuito, ent√£o o custo √© zero.
  input: config.exp.MODEL_PRICING.input,
  output: config.exp.MODEL_PRICING.output,
};

// --- Carregamento √önico de M√≠dia ---
const stickerOptions = mediaCatalog.stickers.map(sticker => sticker.file);
const audioOptions = mediaCatalog.audios.map(audio => audio.file);
const memeOptions = mediaCatalog.memes.map(meme => meme.file);

/**
 * Estima o n√∫mero de tokens para um determinado texto.
 */
function calculateTokens(text: string): number {
  // O 'tiktoken' √© espec√≠fico para modelos OpenAI. Usamos uma aproxima√ß√£o
  // gen√©rica (caracteres / 4) para evitar erros com outros modelos.
  return Math.ceil(text.length / 4);
}

/**
 * Formata os dados de contexto (resumo e opini√µes) para o prompt da IA.
 */
const formatDataForPrompt = (data: Data): string => {
  let formattedData = "Resumo da conversa e opini√µes dos usu√°rios:\n\n";
  if (data.summary) {
    formattedData += `üìã RESUMO DA CONVERSA:\n${data.summary}\n\n`;
  }
  if (data.opinions && data.opinions.length > 0) {
    formattedData += `üë• OPIN√ïES SOBRE OS USU√ÅRIOS:\n`;
    data.opinions.forEach((opinion) => {
      let opinionText = "NEUTRO/MISTO";
      if (opinion.opinion < 20) opinionText = "ODEIO ELE";
      else if (opinion.opinion < 40) opinionText = "N√ÉO GOSTO";
      else if (opinion.opinion < 60) opinionText = "NEUTRO/MISTO";
      else if (opinion.opinion < 80) opinionText = "GOSTO BASTANTE";
      else if (opinion.opinion <= 100) opinionText = "APAIXONADA";
      formattedData += `‚Ä¢ ${opinion.name} (${opinion.jid}):\n`;
      formattedData += `  - N√≠vel de opini√£o: ${opinion.opinion}/100 (${opinionText})\n`;
      if (opinion.traits?.length) {
        formattedData += `  - Caracter√≠sticas: ${opinion.traits.join(", ")}\n`;
      }
      formattedData += "\n";
    });
  }
  return formattedData.trim();
};

// --- Schema da Resposta da IA ---
const RESPONSE_SCHEMA = {
  type: "json_schema" as const,
  json_schema: {
    name: "bot_response",
    strict: false,
    schema: {
      type: "object",
      properties: {
        actions: {
          type: "array",
          minItems: 1,
          items: {
            type: "object",
            properties: {
              type: {
                type: "string",
                enum: ["message", "sticker", "audio", "poll", "location", "meme", "contact"],
              },
              message: {
                type: "object",
                properties: {
                  reply: { type: "string" },
                  text: { type: "string", description: "Resposta ir√¥nica (m√°ximo 300 caracteres)" },
                },
                required: ["text"],
              },
              sticker: { type: "string", enum: stickerOptions },
              audio: { type: "string", enum: audioOptions },
              meme: { type: "string", enum: memeOptions },
              poll: {
                type: "object",
                properties: {
                  question: { type: "string" },
                  options: { type: "array", items: { type: "string" }, minItems: 3, maxItems: 3 },
                },
                required: ["question", "options"],
              },
              location: {
                type: "object",
                properties: { latitude: { type: "number" }, longitude: { type: "number" } },
                required: ["latitude", "longitude"],
              },
              contact: {
                type: "object",
                properties: { name: { type: "string" }, cell: { type: "string" } },
                required: ["cell"],
              },
            },
            required: ["type"],
          },
        },
      },
      required: ["actions"],
    },
  },
};

export default async function generateResponse(
  data: Data,
  messages: Message,
  sessionId: string
): Promise<GenerateResponseResult> {
  beautifulLogger.aiGeneration("start", "Iniciando gera√ß√£o de resposta...");
  const messagesMaped = messages
    .map((message) => `${message.name}: ${message.content}`)
    .join("\n");

  beautifulLogger.aiGeneration("processing", {
    "mensagens processadas": messages.length,
    "mensagem mais recente": messages.at(-1)?.content || "nenhuma",
  });

  let mediaContext = "INFORMA√á√ïES SOBRE M√çDIAS DISPON√çVEIS PARA USO:\n\n";
  mediaContext += "STICKERS DISPON√çVEIS:\n";
  mediaCatalog.stickers.forEach(s => {
    mediaContext += `- arquivo: "${s.file}", descri√ß√£o: "${s.description}"\n`;
  });
  mediaContext += "\n√ÅUDIOS DISPON√çVEIS:\n";
  mediaCatalog.audios.forEach(a => {
    mediaContext += `- arquivo: "${a.file}", descri√ß√£o: "${a.description}"\n`;
  });
  mediaContext += "\nMEMES DISPON√çVEIS:\n";
  mediaCatalog.memes.forEach(m => {
    mediaContext += `- arquivo: "${m.file}", descri√ß√£o: "${m.description}"\n`;
  });

  // Selecionamos apenas os dados do grupo atual (sessionId) do nosso banco de dados.
  // Se n√£o houver dados, passamos um objeto vazio.
  const groupData = data[sessionId] || {};
  const contextData = formatDataForPrompt(groupData);

  const inputMessages: ChatCompletionMessageParam[] = [
    { role: "system", content: PERSONALITY_PROMPT },
    { role: "assistant", content: `${contextData}\n\n${mediaContext}` },
    { role: "user", content: `Conversa: \n\n${messagesMaped}` },
  ];

  const inputText = inputMessages.map((msg) => msg.content || '').join("\n");
  const inputTokens = calculateTokens(inputText);

  beautifulLogger.aiGeneration("tokens", { "tokens de entrada (estimado)": inputTokens });
  beautifulLogger.aiGeneration("processing", "Enviando requisi√ß√£o para a IA...");

  try {
    const response = await openai.chat.completions.create({
      model: MODEL_NAME,
      messages: inputMessages,
      response_format: RESPONSE_SCHEMA,
      temperature: 0.8,
      max_tokens: 200, // Aumentei para dar mais liberdade √† IA
    }, {
      // 2¬∫ Argumento: As op√ß√µes da requisi√ß√£o
      timeout: 30 * 1000, // 30 segundos
    });

    if (!response.choices || response.choices.length === 0 || !response.choices[0].message) {
      throw new Error("A IA n√£o retornou nenhuma op√ß√£o de resposta (array 'choices' vazio).");
    }

    const content = response.choices[0]?.message?.content;
    if (!content) {
      throw new Error("Nenhuma resposta foi gerada pela IA (conte√∫do vazio).");
    }

    const outputTokens = calculateTokens(content);
    const totalTokens = inputTokens + outputTokens;
    const cost = (inputTokens * MODEL_PRICING.input / 100000 / 2) + (outputTokens * MODEL_PRICING.output / 100000 / 2);
    const costMessage = cost === 0 ? `$${cost.toFixed(8)} (modelo gratuito)` : `$${cost.toFixed(8)}`;

    beautifulLogger.aiGeneration("cost", {
      "modelo utilizado": MODEL_NAME,
      "tokens entrada (est.)": inputTokens,
      "tokens sa√≠da (est.)": outputTokens,
      "tokens total (est.)": totalTokens,
      "custo (USD)": costMessage,
    });
    
    // Usamos a express√£o regular para encontrar o trecho de JSON, possiveis formata√ß√µes de markdown.
    const jsonMatch = content.match(/\{[\s\S]*\}/);
    if (!jsonMatch || !jsonMatch[0]) {
      // Se n√£o encontrarmos nenhum trecho de JSON, lan√ßamos um erro claro.
      throw new Error("Nenhum bloco JSON v√°lido foi encontrado na resposta da IA.");
    }

    const jsonString = jsonMatch[0];
    const parsedResponse = JSON.parse(jsonString) as { actions: BotResponse };

    if (!Array.isArray(parsedResponse.actions)) {
      throw new Error("O JSON extra√≠do n√£o cont√©m um array de 'actions' v√°lido.");
    }

    beautifulLogger.aiGeneration("complete", {
      "a√ß√µes processadas": parsedResponse.actions.length,
      "tipos de a√ß√£o": parsedResponse.actions.map((a) => a.type).join(", "),
    });
    
    return { actions: parsedResponse.actions, cost: { inputTokens, outputTokens, totalTokens, cost } };

  } catch (error: any) {
    // ALTERA√á√ÉO 2: Capturamos o erro e o logamos de forma mais detalhada
    beautifulLogger.aiGeneration("error", {
      erro: "Falha cr√≠tica na chamada da API ou na an√°lise da resposta.",
      "mensagem de erro": error.message,
    });
    // Lan√ßamos o erro para que o 'catch' principal em rapy.ts possa lidar com ele.
    throw error;
  }
}