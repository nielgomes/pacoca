//src/inteligence/generateResponse.ts
import { ChatCompletionMessageParam } from "openai/resources";
import * as fs from "fs";
import { openai } from "../services/openai";
import { Data } from "../utils/database";
import PERSONALITY_PROMPT from "../constants/PERSONALITY_PROMPT";
import beautifulLogger from "../utils/beautifulLogger";
import config from '../../model.json';
import mediaCatalog from '../../media_catalog.json';
import { Message, Action, BotResponse, GenerateResponseResult } from "./types";


// --- Constantes Centralizadas ---
// ATUALIZA√á√ÉO: Alterado para o modelo gratuito DeepSeek da OpenRouter.
const MODEL_NAME = config.xai.MODEL_NAME;
const MODEL_PRICING = {
  // Geralmente informado em USD$ por Milh√£o de tokens
  // se o modelo √© gratuito, ent√£o o custo √© zero.
  input: config.xai.MODEL_PRICING.input,
  output: config.xai.MODEL_PRICING.output,
};

// --- Carregamento √önico de M√≠dia ---
const stickerOptions = mediaCatalog.stickers.map(sticker => sticker.file);
const audioOptions = mediaCatalog.audios.map(audio => audio.file);
const memeOptions = mediaCatalog.memes.map(meme => meme.file);

/**
 * Estima o n√∫mero de tokens para um determinado texto.
 */
function calculateTokens(text: string): number {
  // O 'tiktoken' √© espec√≠fico para modelos OpenAI. Usamos uma aproxima√ß√£o
  // gen√©rica (caracteres / 4) para evitar erros com outros modelos.
  return Math.ceil(text.length / 4);
}

/**
 * Formata os dados de contexto (resumo e opini√µes) para o prompt da IA.
 */
const formatDataForPrompt = (data: Data): string => {
  let formattedData = "Resumo da conversa e opini√µes dos usu√°rios:\n\n";
  if (data.summary) {
    formattedData += `üìã RESUMO DA CONVERSA:\n${data.summary}\n\n`;
  }
  if (data.opinions && data.opinions.length > 0) {
    formattedData += `üë• OPIN√ïES SOBRE OS USU√ÅRIOS:\n`;
    data.opinions.forEach((opinion) => {
      let opinionText = "NEUTRO/MISTO";
      if (opinion.opinion < 20) opinionText = "ODEIO ELE";
      else if (opinion.opinion < 40) opinionText = "N√ÉO GOSTO";
      else if (opinion.opinion < 60) opinionText = "NEUTRO/MISTO";
      else if (opinion.opinion < 80) opinionText = "GOSTO BASTANTE";
      else if (opinion.opinion <= 100) opinionText = "APAIXONADA";
      formattedData += `‚Ä¢ ${opinion.name} (${opinion.jid}):\n`;
      formattedData += `  - N√≠vel de opini√£o: ${opinion.opinion}/100 (${opinionText})\n`;
      if (opinion.traits?.length) {
        formattedData += `  - Caracter√≠sticas: ${opinion.traits.join(", ")}\n`;
      }
      formattedData += "\n";
    });
  }
  return formattedData.trim();
};

// --- Schema da Resposta da IA ---
const RESPONSE_SCHEMA = {
  type: "json_schema" as const,
  json_schema: {
    name: "bot_response",
    strict: false,
    schema: {
      type: "object",
      properties: {
        actions: {
          type: "array",
          minItems: 1,
          items: {
            type: "object",
            properties: {
              type: {
                type: "string",
                enum: ["message", "sticker", "audio", "poll", "location", "meme", "contact"],
              },
              message: {
                type: "object",
                properties: {
                  reply: { type: "string" },
                  text: { type: "string", description: "Resposta ir√¥nica (m√°ximo 300 caracteres)" },
                },
                required: ["text"],
              },
              sticker: { type: "string", enum: stickerOptions },
              audio: { type: "string", enum: audioOptions },
              meme: { type: "string", enum: memeOptions },
              poll: {
                type: "object",
                properties: {
                  question: { type: "string" },
                  options: { type: "array", items: { type: "string" }, minItems: 3, maxItems: 3 },
                },
                required: ["question", "options"],
              },
              location: {
                type: "object",
                properties: { latitude: { type: "number" }, longitude: { type: "number" } },
                required: ["latitude", "longitude"],
              },
              contact: {
                type: "object",
                properties: { name: { type: "string" }, cell: { type: "string" } },
                required: ["cell"],
              },
            },
            required: ["type"],
          },
        },
      },
      required: ["actions"],
    },
  },
};

export default async function generateResponse(
  data: Data,
  messages: Message,
  sessionId: string
): Promise<GenerateResponseResult> {
  beautifulLogger.aiGeneration("start", "Iniciando gera√ß√£o de resposta...");
  const messagesMaped = messages
    .map((message) => `${message.name}: ${message.content}`)
    .join("\n");

  beautifulLogger.aiGeneration("processing", {
    "mensagens processadas": messages.length,
    "mensagem mais recente": messages.at(-1)?.content || "nenhuma",
  });

  let mediaContext = "INFORMA√á√ïES SOBRE M√çDIAS DISPON√çVEIS PARA USO:\n\n";
  mediaContext += "STICKERS DISPON√çVEIS:\n";
  mediaCatalog.stickers.forEach(s => {
    mediaContext += `- arquivo: "${s.file}", descri√ß√£o: "${s.description}"\n`;
  });
  mediaContext += "\n√ÅUDIOS DISPON√çVEIS:\n";
  mediaCatalog.audios.forEach(a => {
    mediaContext += `- arquivo: "${a.file}", descri√ß√£o: "${a.description}"\n`;
  });
  mediaContext += "\nMEMES DISPON√çVEIS:\n";
  mediaCatalog.memes.forEach(m => {
    mediaContext += `- arquivo: "${m.file}", descri√ß√£o: "${m.description}"\n`;
  });

  // Selecionamos apenas os dados do grupo atual (sessionId) do nosso banco de dados.
  // Se n√£o houver dados, passamos um objeto vazio.
  const groupData = data[sessionId] || {};
  const contextData = formatDataForPrompt(groupData);

  const inputMessages: ChatCompletionMessageParam[] = [
    { role: "system", content: PERSONALITY_PROMPT },
    { role: "assistant", content: `${contextData}\n\n${mediaContext}` },
    { role: "user", content: `Conversa: \n\n${messagesMaped}` },
  ];

  const inputText = inputMessages.map((msg) => msg.content || '').join("\n");
  const inputTokens = calculateTokens(inputText);

  beautifulLogger.aiGeneration("tokens", { "tokens de entrada (estimado)": inputTokens });
  beautifulLogger.aiGeneration("processing", "Enviando requisi√ß√£o para a IA...");

  const response = await openai.chat.completions.create({
    model: MODEL_NAME,
    messages: inputMessages,
    response_format: RESPONSE_SCHEMA,
    temperature: 0.8,
    max_tokens: 200, // Aumentei um pouco para dar mais liberdade √† IA
  });

  const content = response.choices[0]?.message?.content;
  if (!content) {
    beautifulLogger.aiGeneration("error", "Nenhuma resposta foi gerada pela IA.");
    throw new Error("Nenhuma resposta foi gerada pela IA.");
  }

  const outputTokens = calculateTokens(content);
  const totalTokens = inputTokens + outputTokens;
  const cost = (inputTokens * MODEL_PRICING.input / 100000 / 2) + (outputTokens * MODEL_PRICING.output / 100000 / 2);

  const costMessage = cost === 0 ? `$${cost.toFixed(8)} (modelo gratuito)` : `$${cost.toFixed(8)}`;

  beautifulLogger.aiGeneration("cost", {
    "modelo utilizado": MODEL_NAME,
    "tokens entrada (est.)": inputTokens,
    "tokens sa√≠da (est.)": outputTokens,
    "tokens total (est.)": totalTokens,
    "custo (USD)": costMessage,
  });

  try {
    const parsedResponse = JSON.parse(content) as { actions: BotResponse };
    const actions = parsedResponse.actions;

    if (!Array.isArray(actions) || actions.length === 0) {
      beautifulLogger.aiGeneration("error", "Resposta n√£o cont√©m um array de a√ß√µes v√°lido.");
      throw new Error("Resposta da IA n√£o cont√©m a√ß√µes v√°lidas.");
    }

    beautifulLogger.aiGeneration("complete", {
      "a√ß√µes processadas": actions.length,
      "tipos de a√ß√£o": actions.map((a) => a.type).join(", "),
    });

    return { actions, cost: { inputTokens, outputTokens, totalTokens, cost } };
  } catch (error) {
    beautifulLogger.aiGeneration("error", {
      erro: "Falha ao analisar a resposta JSON da IA.",
      "conte√∫do recebido": content,
    });
    throw new Error("Resposta da IA n√£o est√° no formato JSON v√°lido.");
  }
}